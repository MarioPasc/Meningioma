# Linear Super Learner Visualization Guide

This document describes the comprehensive visualizations generated by `analyze_linear_superlearner_results.py`.

## Overview

The analysis script now generates **7 types of plots** organized into three categories:

### 1. Per-Condition Panels (`panels/PANEL_spacing-{s}_pulse-{p}.png`)

Each condition (spacing × pulse combination) gets a comprehensive 9-subplot panel showing:

#### Row 1: Core Performance
- **A. Overall Loss**: Train vs Test MSE with individual fold points
- **B. Test Performance**: Normalized MAE, PSNR, and Correlation scores
- **C. Ensemble Weights**: Mean weight per model with fold-level variation

#### Row 2: Diagnostics
- **D. Optimization**: Normalized iteration count and weight entropy
- **E. Model Diversity**: Diversity score and 1-correlation metric
- **F. Loss Components**: Image loss vs gradient loss breakdown

#### Row 3: Detailed Analysis
- **G. Weight Composition** (full width): Stacked bar chart showing weight distribution across all folds

### 2. Global Summary Plots

#### `WEIGHTS_heatmap.png`
- **Type**: Annotated heatmap (seaborn)
- **Content**: Mean weights for each model across all conditions
- **Color scheme**: Red-Yellow-Green (0 to 1)
- **Features**: 
  - Numerical annotations (3 decimal places)
  - Grid lines for clarity
  - Clear axis labels

#### `LOSS_overview.png` (2 subplots)
- **Left**: Train vs Test MSE comparison across conditions
- **Right**: Generalization gap analysis (Test - Train)
  - Green: gap < 0.01 (excellent)
  - Yellow: 0.01 ≤ gap < 0.05 (good)
  - Red: gap ≥ 0.05 (overfitting concern)

### 3. Diagnostic Plots

#### `OPTIMIZATION_diagnostics.png` (2×2 grid)

**A. Model Diversity vs Weight Distribution**
- Scatter plot: Avg pairwise correlation vs weight entropy
- Color-coded by diversity score
- **Interpretation**: 
  - High correlation + low entropy → models too similar, uniform weights
  - Low correlation + high entropy → diverse models, informative weighting

**B. Optimization Matrix Conditioning**
- Histogram of log₁₀(condition number)
- Red dashed line: median value
- **Interpretation**: 
  - Condition number < 10³: well-conditioned
  - Condition number > 10⁶: numerical instability risk

**C. Optimization Convergence Speed**
- Bar chart of mean iterations per condition
- Error bars show std across folds
- **Interpretation**: More iterations may indicate difficult optimization landscape

**D. Optimization Success Rate**
- Color-coded bars:
  - Green: 100% success
  - Yellow: 90-100% success
  - Red: <90% success
- Green dashed line at 100% for reference

#### `PERFORMANCE_metrics.png` (dynamic grid)

Generates subplots for each available metric:
- **MSE**: Mean Squared Error
- **MAE**: Mean Absolute Error
- **PSNR**: Peak Signal-to-Noise Ratio
- **Correlation**: Pearson correlation with ground truth
- **RMSE**: Root Mean Squared Error

Each subplot shows:
- Train vs Test comparison
- Mean ± std across folds
- Condition-level grouping

#### `MODEL_diversity.png` (1×2 layout)

**Left: Diversity Score by Condition**
- Bar chart with color coding:
  - Green: diversity > 0.5 (high)
  - Yellow: 0.3 < diversity ≤ 0.5 (moderate)
  - Red: diversity ≤ 0.3 (low)
- Reference lines for thresholds

**Right: Model Similarity vs Weight Distribution**
- Scatter plot per condition
- X-axis: Avg pairwise model correlation
- Y-axis: Weight entropy
- Horizontal line: Maximum possible entropy (log M)

**Interpretation**:
- High diversity score → models make different errors, ensemble beneficial
- Low diversity score → models similar, uniform weights expected

#### `LOSS_decomposition.png` (1×2 layout)

**Left: Train Loss Decomposition**
- Image loss vs gradient loss components
- Shows relative contribution of each term

**Right: Test Loss Decomposition**
- Same as train, for generalization assessment

**Interpretation**:
- High gradient loss → ensemble struggles with edge/detail preservation
- Image/gradient ratio → trade-off between smoothness and sharpness

## Scientific Styling

All plots use:
- **Font**: Times (serif) for publication quality
- **DPI**: 300 for high-resolution output
- **Style**: scienceplots if available, fallback to clean defaults
- **LaTeX**: Enabled when available for mathematical notation
- **Grid**: Semi-transparent dashed lines
- **Error bars**: Displayed with caps
- **Individual points**: Shown with jitter to reveal fold-level variation

## Usage

```bash
python src/mgmGrowth/tasks/superresolution/visualization/analyze_linear_superlearner_results.py \
    --out-root /path/to/results \
    --outdir /path/to/figures  # optional, defaults to <out-root>/LINEAR_SUPER_LEARNER/analysis
```

## Output Directory Structure

```
analysis/
├── panels/
│   ├── PANEL_spacing-3mm_pulse-t1c.png
│   ├── PANEL_spacing-3mm_pulse-t1n.png
│   └── ...
├── WEIGHTS_heatmap.png
├── LOSS_overview.png
├── OPTIMIZATION_diagnostics.png
├── PERFORMANCE_metrics.png
├── MODEL_diversity.png
├── LOSS_decomposition.png
├── aggregated_metrics.csv
└── constraint_violations.csv  # only if violations detected
```

## Diagnostic Workflow

1. **Start with `LOSS_overview.png`**: Check generalization gap
   - Large gaps → reduce model complexity or increase regularization

2. **Check `WEIGHTS_heatmap.png`**: Identify dominant models
   - Uniform weights [0.25, 0.25, 0.25, 0.25] → investigate diversity

3. **If uniform weights, examine**:
   - `MODEL_diversity.png` → Are models too correlated?
   - `OPTIMIZATION_diagnostics.png` → Matrix conditioning issues?
   - `PERFORMANCE_metrics.png` → Do individual models perform similarly?

4. **For performance analysis**:
   - `PERFORMANCE_metrics.png` → Compare MAE, PSNR, correlation
   - `LOSS_decomposition.png` → Balance of image vs gradient terms
   - Per-condition `panels/` → Detailed fold-level breakdown

5. **For optimization issues**:
   - `OPTIMIZATION_diagnostics.png` → Check convergence and conditioning
   - High condition number → increase `--lambda-ridge`
   - Low entropy → models lack diversity, consider different base models

## Key Metrics Interpretation

### Weight Entropy
- **Formula**: H = -Σ wᵢ log(wᵢ)
- **Range**: [0, log M] where M = number of models
- **Interpretation**:
  - H ≈ log M → uniform distribution (all models weighted equally)
  - H ≈ 0 → one model dominates
  - Intermediate → selective combination

### Diversity Score
- **Formula**: D = 1 - avg(ρᵢⱼ) where ρ = pairwise correlation
- **Range**: [0, 1]
- **Interpretation**:
  - D → 1: models make uncorrelated errors (ideal for ensembling)
  - D → 0: models make same errors (ensemble offers little benefit)

### Condition Number
- **Formula**: κ(Q) = λₘₐₓ / λₘᵢₙ
- **Interpretation**:
  - κ < 10³: well-conditioned (good)
  - 10³ ≤ κ < 10⁶: moderately conditioned (acceptable)
  - κ ≥ 10⁶: ill-conditioned (numerical issues likely)

## Troubleshooting

### Plot Missing: "Metrics N/A"
- **Cause**: Corresponding metrics not in CSV
- **Solution**: Re-run training with latest version

### Plot Missing: "Loss Decomp N/A"
- **Cause**: `lambda_grad = 0` (gradient term disabled)
- **Solution**: Expected behavior when gradient regularization not used

### All weights ≈ 0.25 (uniform)
- **Check**: Model diversity plot
- **Likely cause**: High model correlation
- **Solutions**:
  1. Use more diverse base models
  2. Increase training data
  3. Verify models aren't all the same algorithm with different hyperparameters

### Generalization gap large (red bars)
- **Cause**: Overfitting
- **Solutions**:
  1. Increase `--lambda-ridge`
  2. Increase `--lambda-grad`
  3. Use stronger regularization in base models

## Citation

If you use these visualizations in publications, please cite the relevant methods:
- SSIM/PSNR metrics for image quality
- Ensemble diversity measures
- Condition number for numerical stability assessment
